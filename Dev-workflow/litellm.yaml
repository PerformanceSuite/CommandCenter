# LiteLLM Proxy Configuration
# Maps friendly names to actual model endpoints

model_list:
  # OpenAI Models
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
  - model_name: gpt-4-turbo
    litellm_params:
      model: openai/gpt-4-turbo
  
  # Anthropic Claude Models  
  - model_name: claude-3-opus
    litellm_params:
      model: anthropic/claude-3-opus-20240229
  - model_name: claude-3-5-sonnet
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20241022
  - model_name: claude-3-haiku
    litellm_params:
      model: anthropic/claude-3-haiku-20240307
  
  # Google Gemini Models
  - model_name: gemini-1.5-pro
    litellm_params:
      model: gemini/gemini-1.5-pro
  - model_name: gemini-1.5-flash
    litellm_params:
      model: gemini/gemini-1.5-flash
  
  # Groq Models (Super Fast)
  - model_name: groq-llama3.1-70b
    litellm_params:
      model: groq/llama-3.1-70b-versatile
  - model_name: groq-llama3.1-8b
    litellm_params:
      model: groq/llama-3.1-8b-instant
  - model_name: groq-mixtral
    litellm_params:
      model: groq/mixtral-8x7b-32768
  
  # OpenRouter Models
  - model_name: openrouter-claude-3-opus
    litellm_params:
      model: openrouter/anthropic/claude-3-opus
  - model_name: openrouter-gpt-4
    litellm_params:
      model: openrouter/openai/gpt-4
  
  # Local Ollama Models (when Ollama is running)
  - model_name: ollama-llama3.1-8b
    litellm_params:
      model: ollama/llama3.1:8b
      api_base: http://host.docker.internal:11434
  - model_name: ollama-qwen2.5-7b
    litellm_params:
      model: ollama/qwen2.5:7b
      api_base: http://host.docker.internal:11434

# Router settings for reliability
router_settings:
  num_retries: 2
  timeout: 120
  # Fallback if primary model fails
  fallback_models:
    gpt-4o:
      - claude-3-5-sonnet
      - gemini-1.5-pro
    claude-3-opus:
      - gpt-4o
      - gemini-1.5-pro

# General settings
general_settings:
  master_key: "sk-1234"  # You can change this
  
# Logging
litellm_settings:
  drop_params: true
  set_verbose: false
  telemetry: false
