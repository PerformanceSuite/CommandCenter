name: ${COMPOSE_PROJECT_NAME:-commandcenter}

services:
  postgres:
    # Custom Postgres with pgvector extension
    # Build with: python backend/scripts/build-postgres.py
    # Or use pre-built image: commandcenter-postgres:latest
    image: commandcenter-postgres:latest
    build:
      context: ./backend
      dockerfile: Dockerfile.postgres
    container_name: ${COMPOSE_PROJECT_NAME:-commandcenter}_db
    environment:
      POSTGRES_DB: commandcenter
      POSTGRES_USER: commandcenter
      POSTGRES_PASSWORD: ${DB_PASSWORD:-changeme}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U commandcenter"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - commandcenter

  redis:
    image: redis:7-alpine
    container_name: ${COMPOSE_PROJECT_NAME:-commandcenter}_redis
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - commandcenter

  nats:
    image: nats:2.10-alpine
    container_name: ${COMPOSE_PROJECT_NAME:-commandcenter}_nats
    ports:
      - "${NATS_PORT:-4222}:4222"
      - "${NATS_HTTP_PORT:-8222}:8222"  # HTTP monitoring
    command: ["-js", "-m", "8222"]  # Enable JetStream and monitoring
    networks:
      - commandcenter

  orchestration:
    build:
      context: ./hub/orchestration
      dockerfile: Dockerfile
    container_name: ${COMPOSE_PROJECT_NAME:-commandcenter}_orchestration
    environment:
      DATABASE_URL: postgresql://commandcenter:${DB_PASSWORD:-changeme}@postgres:5432/commandcenter
      NATS_URL: nats://nats:4222
      PORT: 9002
      NODE_ENV: production
      LOG_LEVEL: info
      AGENT_MAX_MEMORY_MB: 512
      AGENT_TIMEOUT_SECONDS: 300
    ports:
      - "${ORCHESTRATION_PORT:-9002}:9002"
    depends_on:
      postgres:
        condition: service_healthy
      nats:
        condition: service_started
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock  # For Dagger SDK
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "node -e \"require('http').get('http://localhost:9002/health', (r) => process.exit(r.statusCode === 200 ? 0 : 1))\""]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - commandcenter

  backend:
    build:
      context: .  # Build from repo root to access libs/
      dockerfile: backend/Dockerfile
    container_name: ${COMPOSE_PROJECT_NAME:-commandcenter}_backend
    env_file: .env.docker
    volumes:
      - rag_storage:/app/rag_storage
      - ./backend/output:/app/output
    ports:
      - "${BACKEND_PORT:-8000}:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health').read()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - commandcenter

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      target: production
      args:
        # Build-time default (used during npm build, overridden at runtime by entrypoint)
        VITE_PROJECT_NAME: "Command Center Template"
        VITE_API_BASE_URL: ${VITE_API_BASE_URL:-}
    container_name: ${COMPOSE_PROJECT_NAME:-commandcenter}_frontend
    environment:
      VITE_API_URL: http://localhost:${BACKEND_PORT:-8000}
      # Runtime value (dynamically injected into /config.js by docker-entrypoint.sh)
      # This allows each project instance to display its own name without rebuilding
      VITE_PROJECT_NAME: ${VITE_PROJECT_NAME:-Command Center}
    ports:
      - "${FRONTEND_PORT:-3000}:80"
    depends_on:
      backend:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "test -f /usr/share/nginx/html/index.html"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - commandcenter

  celery-worker:
    build:
      context: .  # Build from repo root to access libs/
      dockerfile: backend/Dockerfile
    container_name: ${COMPOSE_PROJECT_NAME:-commandcenter}_celery_worker
    command: celery -A app.tasks worker --loglevel=info --concurrency=4
    env_file: .env.docker
    volumes:
      - rag_storage:/app/rag_storage
      - ./backend/output:/app/output
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - commandcenter

  celery-beat:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: ${COMPOSE_PROJECT_NAME:-commandcenter}_celery_beat
    command: celery -A app.tasks beat --loglevel=info
    env_file: .env.docker
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - commandcenter

  flower:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: ${COMPOSE_PROJECT_NAME:-commandcenter}_flower
    command: celery -A app.tasks flower --port=5555
    ports:
      - "${FLOWER_PORT:-5555}:5555"
    environment:
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - commandcenter

volumes:
  postgres_data:
    driver: local
  rag_storage:
    driver: local
  redis_data:
    driver: local

networks:
  commandcenter:
    driver: bridge
