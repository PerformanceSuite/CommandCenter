# Phase 10 Phase 5: Observability Stack - Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Implement production-grade observability for CommandCenter agent orchestration using OpenTelemetry, Grafana, Prometheus, and Tempo.

**Architecture:** OpenTelemetry SDK instruments TypeScript service â†’ OTEL Collector processes telemetry â†’ Tempo (traces) + Prometheus (metrics) + Loki (logs) â†’ Grafana (dashboards & alerts)

**Tech Stack:** OpenTelemetry SDK (Node.js), Grafana Tempo 2.3, Prometheus 2.48, Grafana Loki 2.9, Grafana 10.2, Docker Compose

**Design Reference:** `docs/plans/2025-11-19-phase-10-phase-5-observability-design.md`

---

## Phase 1: Foundation (Auto-Instrumentation)

### Task 1: Install OpenTelemetry Dependencies

**Files:**
- Modify: `hub/orchestration/package.json`

**Step 1: Add OpenTelemetry packages**

```bash
cd hub/orchestration
npm install @opentelemetry/sdk-node \
  @opentelemetry/auto-instrumentations-node \
  @opentelemetry/exporter-trace-otlp-grpc \
  @opentelemetry/exporter-prometheus \
  @opentelemetry/api \
  @opentelemetry/resources \
  @opentelemetry/semantic-conventions
```

**Step 2: Verify installation**

Run: `npm list | grep opentelemetry`
Expected: Shows 7 @opentelemetry packages installed

**Step 3: Commit**

```bash
git add package.json package-lock.json
git commit -m "feat(observability): Add OpenTelemetry dependencies"
```

---

### Task 2: Create Instrumentation Module

**Files:**
- Create: `hub/orchestration/src/instrumentation.ts`

**Step 1: Write instrumentation setup**

```typescript
// hub/orchestration/src/instrumentation.ts
import { NodeSDK } from '@opentelemetry/sdk-node';
import { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-grpc';
import { PrometheusExporter } from '@opentelemetry/exporter-prometheus';
import { Resource } from '@opentelemetry/resources';
import { SemanticResourceAttributes } from '@opentelemetry/semantic-conventions';

const sdk = new NodeSDK({
  resource: new Resource({
    [SemanticResourceAttributes.SERVICE_NAME]: 'orchestration-service',
    [SemanticResourceAttributes.SERVICE_VERSION]: process.env.npm_package_version || '1.0.0',
  }),
  traceExporter: new OTLPTraceExporter({
    url: process.env.OTEL_EXPORTER_OTLP_ENDPOINT || 'http://localhost:4317',
  }),
  metricReader: new PrometheusExporter({
    port: 9464,
    endpoint: '/metrics',
  }),
  instrumentations: [
    getNodeAutoInstrumentations({
      '@opentelemetry/instrumentation-express': { enabled: true },
      '@opentelemetry/instrumentation-http': { enabled: true },
      '@opentelemetry/instrumentation-fs': { enabled: false }, // Too noisy
    }),
  ],
});

sdk.start();
console.log('[OpenTelemetry] Instrumentation started');

// Graceful shutdown
process.on('SIGTERM', () => {
  sdk
    .shutdown()
    .then(() => console.log('[OpenTelemetry] Shutdown complete'))
    .catch((error) => console.error('[OpenTelemetry] Shutdown error', error))
    .finally(() => process.exit(0));
});

export default sdk;
```

**Step 2: Commit**

```bash
git add src/instrumentation.ts
git commit -m "feat(observability): Create OpenTelemetry instrumentation module"
```

---

### Task 3: Import Instrumentation First

**Files:**
- Modify: `hub/orchestration/src/index.ts:1-10`

**Step 1: Read current index.ts**

```bash
head -20 hub/orchestration/src/index.ts
```

**Step 2: Add instrumentation import as FIRST line**

```typescript
// hub/orchestration/src/index.ts
import './instrumentation'; // MUST be first import for auto-instrumentation

import { startServer } from './api/server';
import { eventBridge } from './events/nats-bridge';
// ... rest of existing imports
```

**Step 3: Commit**

```bash
git add src/index.ts
git commit -m "feat(observability): Import instrumentation first for auto-instrumentation"
```

---

### Task 4: Create OTEL Collector Configuration

**Files:**
- Create: `hub/observability/otel-collector-config.yml`

**Step 1: Create observability directory**

```bash
mkdir -p hub/observability
```

**Step 2: Write OTEL Collector config**

```yaml
# hub/observability/otel-collector-config.yml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  batch:
    timeout: 10s
    send_batch_size: 1024

  resource:
    attributes:
      - key: deployment.environment
        value: ${ENVIRONMENT:-development}
        action: upsert

  attributes:
    actions:
      - key: service.namespace
        value: commandcenter-hub
        action: upsert

exporters:
  otlp/tempo:
    endpoint: tempo:4317
    tls:
      insecure: true

  prometheus:
    endpoint: "0.0.0.0:8889"
    namespace: orchestration

  loki:
    endpoint: http://loki:3100/loki/api/v1/push

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch, resource, attributes]
      exporters: [otlp/tempo]

    metrics:
      receivers: [otlp]
      processors: [batch, resource]
      exporters: [prometheus]

    logs:
      receivers: [otlp]
      processors: [batch, resource]
      exporters: [loki]
```

**Step 3: Commit**

```bash
git add observability/otel-collector-config.yml
git commit -m "feat(observability): Add OpenTelemetry Collector configuration"
```

---

### Task 5: Create Prometheus Configuration

**Files:**
- Create: `hub/observability/prometheus.yml`

**Step 1: Write Prometheus config**

```yaml
# hub/observability/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'commandcenter-hub'
    environment: 'development'

scrape_configs:
  # Orchestration service metrics (from Prometheus exporter)
  - job_name: 'orchestration-service'
    static_configs:
      - targets: ['host.docker.internal:9464']

  # OTEL Collector metrics
  - job_name: 'otel-collector'
    static_configs:
      - targets: ['otel-collector:8888']

  # Prometheus self-monitoring
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
```

**Step 2: Commit**

```bash
git add observability/prometheus.yml
git commit -m "feat(observability): Add Prometheus configuration"
```

---

### Task 6: Create Tempo Configuration

**Files:**
- Create: `hub/observability/tempo.yml`

**Step 1: Write Tempo config**

```yaml
# hub/observability/tempo.yml
server:
  http_listen_port: 3200

distributor:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317

ingester:
  trace_idle_period: 10s
  max_block_bytes: 1_000_000
  max_block_duration: 5m

compactor:
  compaction:
    block_retention: 168h  # 7 days

storage:
  trace:
    backend: local
    wal:
      path: /tmp/tempo/wal
    local:
      path: /tmp/tempo/blocks

query_frontend:
  search:
    duration_slo: 5s
    throughput_bytes_slo: 1.073741824e+09
```

**Step 2: Commit**

```bash
git add observability/tempo.yml
git commit -m "feat(observability): Add Tempo configuration"
```

---

### Task 7: Create Grafana Datasource Provisioning

**Files:**
- Create: `hub/observability/grafana/provisioning/datasources/datasources.yml`

**Step 1: Create Grafana provisioning directories**

```bash
mkdir -p hub/observability/grafana/provisioning/datasources
mkdir -p hub/observability/grafana/provisioning/dashboards
mkdir -p hub/observability/grafana/dashboards
```

**Step 2: Write datasource provisioning config**

```yaml
# hub/observability/grafana/provisioning/datasources/datasources.yml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: false

  - name: Tempo
    type: tempo
    access: proxy
    url: http://tempo:3200
    editable: false
    jsonData:
      tracesToLogs:
        datasourceUid: 'loki'
      serviceMap:
        datasourceUid: 'prometheus'

  - name: Loki
    type: loki
    access: proxy
    url: http://loki:3100
    editable: false
    jsonData:
      derivedFields:
        - datasourceUid: tempo
          matcherRegex: "traceID=(\\w+)"
          name: TraceID
          url: "$${__value.raw}"
```

**Step 3: Commit**

```bash
git add observability/grafana/
git commit -m "feat(observability): Add Grafana datasource provisioning"
```

---

### Task 8: Create Dashboard Provisioning Config

**Files:**
- Create: `hub/observability/grafana/provisioning/dashboards/dashboards.yml`

**Step 1: Write dashboard provisioning config**

```yaml
# hub/observability/grafana/provisioning/dashboards/dashboards.yml
apiVersion: 1

providers:
  - name: 'CommandCenter Dashboards'
    orgId: 1
    folder: 'CommandCenter'
    type: file
    disableDeletion: false
    updateIntervalSeconds: 10
    allowUiUpdates: true
    options:
      path: /var/lib/grafana/dashboards
```

**Step 2: Commit**

```bash
git add observability/grafana/provisioning/dashboards/dashboards.yml
git commit -m "feat(observability): Add Grafana dashboard provisioning config"
```

---

### Task 9: Create Docker Compose Observability Stack

**Files:**
- Create: `hub/docker-compose.observability.yml`

**Step 1: Write Docker Compose file**

```yaml
# hub/docker-compose.observability.yml
version: '3.8'

services:
  # OpenTelemetry Collector
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.91.0
    container_name: hub-otel-collector
    command: ["--config=/etc/otel-collector-config.yml"]
    volumes:
      - ./observability/otel-collector-config.yml:/etc/otel-collector-config.yml
    ports:
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver
      - "8888:8888"   # Metrics endpoint
    environment:
      - ENVIRONMENT=development
    networks:
      - observability

  # Tempo (Distributed Tracing)
  tempo:
    image: grafana/tempo:2.3.1
    container_name: hub-tempo
    command: ["-config.file=/etc/tempo.yml"]
    volumes:
      - ./observability/tempo.yml:/etc/tempo.yml
      - tempo_data:/tmp/tempo
    ports:
      - "3200:3200"   # Tempo HTTP
      - "4317"        # OTLP gRPC (internal)
    networks:
      - observability

  # Prometheus (Metrics)
  prometheus:
    image: prom/prometheus:v2.48.1
    container_name: hub-prometheus-obs
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    volumes:
      - ./observability/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - observability

  # Loki (Logs)
  loki:
    image: grafana/loki:2.9.3
    container_name: hub-loki
    command: -config.file=/etc/loki/local-config.yml
    ports:
      - "3100:3100"
    volumes:
      - loki_data:/loki
    networks:
      - observability

  # Grafana (Visualization)
  grafana:
    image: grafana/grafana:10.2.3
    container_name: hub-grafana-obs
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://localhost:3001
      - GF_AUTH_ANONYMOUS_ENABLED=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
    volumes:
      - grafana_data:/var/lib/grafana
      - ./observability/grafana/provisioning:/etc/grafana/provisioning
      - ./observability/grafana/dashboards:/var/lib/grafana/dashboards
    ports:
      - "3001:3000"
    depends_on:
      - prometheus
      - tempo
      - loki
    networks:
      - observability

volumes:
  tempo_data:
    driver: local
  prometheus_data:
    driver: local
  loki_data:
    driver: local
  grafana_data:
    driver: local

networks:
  observability:
    driver: bridge
```

**Step 2: Commit**

```bash
git add docker-compose.observability.yml
git commit -m "feat(observability): Add Docker Compose observability stack"
```

---

### Task 10: Test Basic Setup

**Files:**
- None (verification step)

**Step 1: Start observability stack**

```bash
cd hub
docker-compose -f docker-compose.observability.yml up -d
```

Expected: All 5 services start (otel-collector, tempo, prometheus, loki, grafana)

**Step 2: Verify services are healthy**

```bash
docker-compose -f docker-compose.observability.yml ps
```

Expected: All services show "Up" status

**Step 3: Start orchestration service**

```bash
cd hub/orchestration
npm run dev
```

Expected: Console shows "[OpenTelemetry] Instrumentation started"

**Step 4: Make test API request**

```bash
curl http://localhost:9002/health
```

Expected: Returns {"status": "ok"}

**Step 5: Check traces in Tempo**

Open browser: http://localhost:3200/explore
Search for service: orchestration-service
Expected: See traces from the /health request

**Step 6: Check metrics in Prometheus**

Open browser: http://localhost:9090
Query: `up{job="orchestration-service"}`
Expected: Shows value = 1 (service is up)

**Step 7: Access Grafana**

Open browser: http://localhost:3001
Login: admin / admin
Expected: Grafana loads with 3 datasources (Prometheus, Tempo, Loki)

**Step 8: Stop services**

```bash
cd hub
docker-compose -f docker-compose.observability.yml down
cd orchestration
# Stop npm run dev (Ctrl+C)
```

**Step 9: Document verification in commit**

```bash
git add -A
git commit -m "feat(observability): Phase 1 complete - Auto-instrumentation working

- OTEL Collector receiving traces
- Tempo storing traces
- Prometheus scraping metrics
- Grafana connected to all datasources
- Verified with /health endpoint trace"
```

---

## Phase 2: Custom Spans (Workflow & Agent Tracking)

### Task 11: Create Workflow Metrics Module

**Files:**
- Create: `hub/orchestration/src/metrics/workflow-metrics.ts`

**Step 1: Write metrics module**

```typescript
// hub/orchestration/src/metrics/workflow-metrics.ts
import { metrics } from '@opentelemetry/api';

const meter = metrics.getMeter('orchestration-service');

// Workflow metrics
export const workflowRunCounter = meter.createCounter('workflow.runs.total', {
  description: 'Total number of workflow runs',
  unit: '1',
});

export const workflowDuration = meter.createHistogram('workflow.duration', {
  description: 'Workflow execution time',
  unit: 'ms',
});

export const activeWorkflows = meter.createUpDownCounter('workflows.active', {
  description: 'Number of currently running workflows',
  unit: '1',
});

export const workflowApprovalWaitTime = meter.createHistogram('workflow.approval.wait_time', {
  description: 'Time spent waiting for approval',
  unit: 'ms',
});

// Agent metrics
export const agentRunCounter = meter.createCounter('agent.runs.total', {
  description: 'Total number of agent executions',
  unit: '1',
});

export const agentDuration = meter.createHistogram('agent.duration', {
  description: 'Agent execution time',
  unit: 'ms',
});

export const agentErrorCounter = meter.createCounter('agent.errors.total', {
  description: 'Total agent execution failures',
  unit: '1',
});

export const agentRetryCounter = meter.createCounter('agent.retry.count', {
  description: 'Number of agent retry attempts',
  unit: '1',
});
```

**Step 2: Commit**

```bash
git add src/metrics/workflow-metrics.ts
git commit -m "feat(observability): Create workflow and agent metrics module"
```

---

### Task 12: Add Workflow Execution Spans

**Files:**
- Modify: `hub/orchestration/src/services/workflow-runner.ts`

**Step 1: Add OpenTelemetry imports**

Add at top of file (after existing imports):

```typescript
import { trace, SpanStatusCode } from '@opentelemetry/api';
import {
  workflowRunCounter,
  workflowDuration,
  activeWorkflows,
} from '../metrics/workflow-metrics';

const tracer = trace.getTracer('workflow-runner');
```

**Step 2: Wrap executeWorkflow with span**

Find the `executeWorkflow` method and wrap with tracing:

```typescript
async executeWorkflow(runId: string): Promise<WorkflowRun> {
  return tracer.startActiveSpan('workflow.execute', async (span) => {
    const startTime = Date.now();

    try {
      const workflowRun = await this.prisma.workflowRun.findUnique({
        where: { id: runId },
        include: { workflow: true },
      });

      if (!workflowRun) {
        span.recordException(new Error('Workflow run not found'));
        span.setStatus({ code: SpanStatusCode.ERROR, message: 'Not found' });
        throw new Error(`Workflow run ${runId} not found`);
      }

      // Add span attributes
      span.setAttribute('workflow.run.id', runId);
      span.setAttribute('workflow.id', workflowRun.workflow.id);
      span.setAttribute('workflow.name', workflowRun.workflow.name);
      span.setAttribute('workflow.trigger', workflowRun.trigger);

      // Increment active workflows
      activeWorkflows.add(1);

      // Update status to RUNNING
      await this.prisma.workflowRun.update({
        where: { id: runId },
        data: { status: 'RUNNING', startedAt: new Date() },
      });

      // Execute workflow nodes (existing logic)
      // ... existing workflow execution code ...

      const duration = Date.now() - startTime;

      // Success - record metrics
      span.setStatus({ code: SpanStatusCode.OK });
      span.setAttribute('workflow.status', 'SUCCESS');
      span.setAttribute('workflow.duration.ms', duration);

      workflowRunCounter.add(1, {
        status: 'SUCCESS',
        workflow_id: workflowRun.workflow.id,
        trigger_type: workflowRun.trigger,
      });

      workflowDuration.record(duration, {
        workflow_id: workflowRun.workflow.id,
        status: 'SUCCESS',
      });

      return await this.prisma.workflowRun.update({
        where: { id: runId },
        data: {
          status: 'SUCCESS',
          finishedAt: new Date(),
        },
      });
    } catch (error) {
      const duration = Date.now() - startTime;

      // Record error
      span.recordException(error);
      span.setStatus({
        code: SpanStatusCode.ERROR,
        message: error.message,
      });
      span.setAttribute('workflow.status', 'FAILED');
      span.setAttribute('error.type', error.constructor.name);

      workflowRunCounter.add(1, {
        status: 'FAILED',
        workflow_id: workflowRun?.workflow?.id || 'unknown',
        trigger_type: workflowRun?.trigger || 'unknown',
      });

      workflowDuration.record(duration, {
        workflow_id: workflowRun?.workflow?.id || 'unknown',
        status: 'FAILED',
      });

      throw error;
    } finally {
      // Decrement active workflows
      activeWorkflows.add(-1);
      span.end();
    }
  });
}
```

**Step 3: Commit**

```bash
git add src/services/workflow-runner.ts
git commit -m "feat(observability): Add workflow execution spans and metrics"
```

---

### Task 13: Add Agent Execution Spans

**Files:**
- Modify: `hub/orchestration/src/services/workflow-runner.ts` (continue from Task 12)

**Step 1: Import agent metrics**

Add to existing imports:

```typescript
import {
  agentRunCounter,
  agentDuration,
  agentErrorCounter,
} from '../metrics/workflow-metrics';
```

**Step 2: Wrap executeAgent with span**

Find the `executeAgent` method (or similar agent execution code) and wrap:

```typescript
async executeAgent(
  workflowRunId: string,
  agentId: string,
  action: string,
  inputsJson: any
): Promise<AgentRun> {
  return tracer.startActiveSpan('agent.execute', async (span) => {
    const startTime = Date.now();

    try {
      // Add span attributes
      span.setAttribute('agent.id', agentId);
      span.setAttribute('agent.action', action);
      span.setAttribute('workflow.run.id', workflowRunId);

      // Create agent run record
      const agentRun = await this.prisma.agentRun.create({
        data: {
          workflowRunId,
          agentId,
          inputJson: inputsJson,
          status: 'RUNNING',
          startedAt: new Date(),
        },
      });

      span.setAttribute('agent.run.id', agentRun.id);

      // Execute agent via Dagger
      const result = await this.daggerExecutor.runAgent(
        agentId,
        action,
        inputsJson
      );

      const duration = Date.now() - startTime;

      // Success
      span.setAttribute('agent.status', 'SUCCESS');
      span.setAttribute('agent.duration.ms', duration);
      span.setStatus({ code: SpanStatusCode.OK });

      agentRunCounter.add(1, {
        agent_id: agentId,
        action,
        status: 'SUCCESS',
      });

      agentDuration.record(duration, {
        agent_id: agentId,
        action,
        status: 'SUCCESS',
      });

      return await this.prisma.agentRun.update({
        where: { id: agentRun.id },
        data: {
          status: 'SUCCESS',
          outputJson: result,
          finishedAt: new Date(),
          durationMs: duration,
        },
      });
    } catch (error) {
      const duration = Date.now() - startTime;

      // Record error
      span.recordException(error);
      span.setStatus({ code: SpanStatusCode.ERROR });
      span.setAttribute('agent.status', 'FAILED');
      span.setAttribute('error.message', error.message);

      agentRunCounter.add(1, {
        agent_id: agentId,
        action,
        status: 'FAILED',
      });

      agentErrorCounter.add(1, {
        agent_id: agentId,
        error_type: error.constructor.name,
      });

      agentDuration.record(duration, {
        agent_id: agentId,
        action,
        status: 'FAILED',
      });

      throw error;
    } finally {
      span.end();
    }
  });
}
```

**Step 3: Commit**

```bash
git add src/services/workflow-runner.ts
git commit -m "feat(observability): Add agent execution spans and metrics"
```

---

### Task 14: Test Custom Spans

**Files:**
- None (verification step)

**Step 1: Start observability stack**

```bash
cd hub
docker-compose -f docker-compose.observability.yml up -d
```

**Step 2: Start orchestration service**

```bash
cd hub/orchestration
npm run dev
```

**Step 3: Trigger test workflow**

```bash
curl -X POST http://localhost:9002/api/workflows/scan-and-notify/trigger \
  -H "Content-Type: application/json" \
  -d '{"contextJson": {"repository": "test-repo"}}'
```

Expected: Returns workflow run ID

**Step 4: Check traces in Tempo**

Open: http://localhost:3200/explore
Search: service.name = "orchestration-service"
Expected: See trace with spans:
- workflow.execute (parent)
- agent.execute (child, repeated for each agent)

**Step 5: Check custom metrics in Prometheus**

Open: http://localhost:9090
Query: `workflow_runs_total`
Expected: Shows counter with labels (status, workflow_id, trigger_type)

Query: `histogram_quantile(0.95, workflow_duration_bucket)`
Expected: Shows p95 workflow duration

**Step 6: Verify span attributes**

In Tempo, click on workflow.execute span
Expected attributes:
- workflow.run.id
- workflow.id
- workflow.name
- workflow.trigger
- workflow.status
- workflow.duration.ms

**Step 7: Document verification**

```bash
git add -A
git commit -m "feat(observability): Phase 2 complete - Custom spans working

- Workflow execution traced end-to-end
- Agent execution spans nested correctly
- Custom metrics appearing in Prometheus
- Span attributes include workflow/agent metadata"
```

---

## Phase 3: Metrics & Dashboards

### Task 15: Create Workflow Overview Dashboard

**Files:**
- Create: `hub/observability/grafana/dashboards/workflow-overview.json`

**Step 1: Create dashboard JSON**

This is a large JSON file. Key panels to include:

```json
{
  "dashboard": {
    "title": "Workflow Overview",
    "panels": [
      {
        "title": "Workflows per Minute",
        "targets": [
          {
            "expr": "sum(rate(workflow_runs_total[1m])) by (status)"
          }
        ],
        "type": "timeseries"
      },
      {
        "title": "Average Workflow Duration (p95)",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(workflow_duration_bucket[5m]))"
          }
        ],
        "type": "gauge"
      },
      {
        "title": "Active Workflows",
        "targets": [
          {
            "expr": "workflows_active"
          }
        ],
        "type": "stat"
      },
      {
        "title": "Workflow Success Rate",
        "targets": [
          {
            "expr": "sum(rate(workflow_runs_total{status=\"SUCCESS\"}[5m])) / sum(rate(workflow_runs_total[5m]))"
          }
        ],
        "type": "stat"
      }
    ]
  }
}
```

**Note**: Full dashboard JSON is ~500 lines. Use Grafana UI to build panels, then export JSON.

**Step 2: Build dashboard in Grafana UI**

1. Start stack: `docker-compose -f docker-compose.observability.yml up -d`
2. Open Grafana: http://localhost:3001
3. Create new dashboard
4. Add panels per design document (section 3.2, Dashboard 1)
5. Export dashboard JSON
6. Save to `hub/observability/grafana/dashboards/workflow-overview.json`

**Step 3: Commit**

```bash
git add observability/grafana/dashboards/workflow-overview.json
git commit -m "feat(observability): Add Workflow Overview dashboard"
```

---

### Task 16: Create Agent Performance Dashboard

**Files:**
- Create: `hub/observability/grafana/dashboards/agent-performance.json`

**Step 1: Create dashboard with key panels**

Key panels (from design doc section 3.2, Dashboard 2):
- Agent Execution Heatmap
- Agent Duration Percentiles (p50, p95, p99)
- Agent Failure Rate
- Most Expensive Agents (table)
- Agent Retry Trends

**Step 2: Build in Grafana UI and export**

1. Open Grafana: http://localhost:3001
2. Create new dashboard "Agent Performance"
3. Add panels with queries from design doc
4. Export JSON
5. Save to `hub/observability/grafana/dashboards/agent-performance.json`

**Step 3: Commit**

```bash
git add observability/grafana/dashboards/agent-performance.json
git commit -m "feat(observability): Add Agent Performance dashboard"
```

---

### Task 17: Create System Health Dashboard

**Files:**
- Create: `hub/observability/grafana/dashboards/system-health.json`

**Step 1: Create dashboard with system metrics**

Key panels (from design doc section 3.2, Dashboard 3):
- API Response Times (p95 by endpoint)
- Database Query Latency
- Error Rate (5xx errors)
- Node.js Memory Usage
- Event Loop Lag
- Active HTTP Connections

**Step 2: Build in Grafana UI and export**

1. Create dashboard "System Health"
2. Add panels for auto-instrumented metrics:
   - `http_server_duration_bucket`
   - `db_client_operation_duration_bucket`
   - `process_runtime_nodejs_memory_heap_bytes`
   - `process_runtime_nodejs_event_loop_lag_milliseconds`
3. Export JSON
4. Save to `hub/observability/grafana/dashboards/system-health.json`

**Step 3: Commit**

```bash
git add observability/grafana/dashboards/system-health.json
git commit -m "feat(observability): Add System Health dashboard"
```

---

### Task 18: Create Cost Tracking Dashboard

**Files:**
- Create: `hub/observability/grafana/dashboards/cost-tracking.json`

**Step 1: Create cost analysis dashboard**

Key panels (from design doc section 3.2, Dashboard 4):
- Agent Execution Minutes (bar chart)
- Workflow Cost Breakdown (pie chart)
- Monthly Execution Trends (time series)
- Cost per Workflow Type (table)

**Step 2: Build in Grafana UI**

1. Create dashboard "Cost Tracking"
2. Add panels with cost-related queries:
   - `sum(agent_duration / 60000) by (agent_id)` (execution minutes)
   - `sum(workflow_duration) by (workflow_id)` (workflow cost breakdown)
3. Export JSON
4. Save to `hub/observability/grafana/dashboards/cost-tracking.json`

**Step 3: Commit**

```bash
git add observability/grafana/dashboards/cost-tracking.json
git commit -m "feat(observability): Add Cost Tracking dashboard"
```

---

### Task 19: Test Dashboard Provisioning

**Files:**
- None (verification step)

**Step 1: Restart observability stack**

```bash
cd hub
docker-compose -f docker-compose.observability.yml down
docker-compose -f docker-compose.observability.yml up -d
```

**Step 2: Wait for Grafana to start**

```bash
sleep 30
curl http://localhost:3001/api/health
```

Expected: Returns {"database": "ok"}

**Step 3: Verify dashboards loaded**

Open: http://localhost:3001/dashboards
Expected: See 4 dashboards in "CommandCenter" folder:
- Workflow Overview
- Agent Performance
- System Health
- Cost Tracking

**Step 4: Trigger test workflow to populate dashboards**

```bash
cd hub/orchestration
npm run dev &
sleep 5
curl -X POST http://localhost:9002/api/workflows/scan-and-notify/trigger \
  -H "Content-Type: application/json" \
  -d '{"contextJson": {"repository": "test-repo"}}'
sleep 10
```

**Step 5: Verify dashboards show data**

Open each dashboard in Grafana
Expected: Panels show data from the test workflow execution

**Step 6: Document verification**

```bash
git add -A
git commit -m "feat(observability): Phase 3 complete - Dashboards working

- 4 dashboards auto-provisioned on Grafana startup
- All panels showing real-time data
- Queries complete in < 1 second
- Dashboard persists after restart"
```

---

## Phase 4: Alerting & Production Readiness

### Task 20: Create Alert Rules Configuration

**Files:**
- Create: `hub/observability/grafana/provisioning/alerting/rules.yml`

**Step 1: Create alerting directory**

```bash
mkdir -p hub/observability/grafana/provisioning/alerting
```

**Step 2: Write alert rules**

```yaml
# hub/observability/grafana/provisioning/alerting/rules.yml
apiVersion: 1

groups:
  - name: workflow_alerts
    interval: 1m
    rules:
      - uid: workflow_failure_rate
        title: Workflow Failure Rate
        condition: B
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: |
                sum(rate(workflow_runs_total{status="FAILED"}[5m]))
                / sum(rate(workflow_runs_total[5m])) > 0.10
          - refId: B
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: threshold
              expression: A
              conditions:
                - evaluator:
                    params: [0.10]
                    type: gt
        labels:
          severity: critical
        annotations:
          summary: "High workflow failure rate"
          description: "More than 10% of workflows are failing"
        for: 5m

      - uid: high_workflow_duration
        title: High Workflow Duration
        condition: B
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              expr: |
                histogram_quantile(0.95,
                  rate(workflow_duration_bucket[5m])) > 300000
          - refId: B
            relativeTimeRange:
              from: 0
              to: 0
            datasourceUid: __expr__
            model:
              type: threshold
              expression: A
              conditions:
                - evaluator:
                    params: [300000]
                    type: gt
        labels:
          severity: warning
        annotations:
          summary: "High workflow duration (p95 > 5 minutes)"
        for: 10m

  - name: system_alerts
    interval: 1m
    rules:
      - uid: service_down
        title: Service Down
        condition: B
        data:
          - refId: A
            datasourceUid: prometheus
            model:
              expr: up{job="orchestration-service"} == 0
          - refId: B
            datasourceUid: __expr__
            model:
              type: threshold
              expression: A
              conditions:
                - evaluator:
                    params: [0]
                    type: eq
        labels:
          severity: critical
        annotations:
          summary: "Orchestration service is down"
        for: 1m

      - uid: high_error_rate
        title: High Error Rate
        condition: B
        data:
          - refId: A
            datasourceUid: prometheus
            model:
              expr: |
                sum(rate(http_server_requests_total{status=~"5.."}[1m])) > 50
          - refId: B
            datasourceUid: __expr__
            model:
              type: threshold
              expression: A
              conditions:
                - evaluator:
                    params: [50]
                    type: gt
        labels:
          severity: critical
        annotations:
          summary: "High 5xx error rate"
        for: 2m
```

**Step 3: Commit**

```bash
git add observability/grafana/provisioning/alerting/rules.yml
git commit -m "feat(observability): Add Grafana alert rules"
```

---

### Task 21: Create Grafana Webhook Receiver

**Files:**
- Create: `hub/orchestration/src/api/routes/webhooks.ts` (if not exists)
- Modify: `hub/orchestration/src/api/routes/webhooks.ts` (add Grafana webhook)

**Step 1: Add Grafana webhook endpoint**

```typescript
// hub/orchestration/src/api/routes/webhooks.ts

import { Router, Request, Response } from 'express';
import { prisma } from '../../prisma-client';
import { workflowRunner } from '../../services/workflow-runner';

const router = Router();

// Grafana alert webhook
router.post('/webhooks/grafana', async (req: Request, res: Response) => {
  try {
    const alert = req.body;

    console.log('[Webhook] Received Grafana alert:', alert.title);

    // Map Grafana alert to workflow context
    const context = {
      severity: alert.state === 'alerting' ? 'critical' : 'warning',
      alert_name: alert.title,
      message: alert.message || alert.title,
      state: alert.state,
      dashboard_url: alert.dashboardURL,
      timestamp: new Date().toISOString(),
      labels: alert.labels || {},
    };

    // Find alert-notification workflow
    const workflow = await prisma.workflow.findFirst({
      where: { name: 'alert-notification' },
    });

    if (!workflow) {
      console.warn('[Webhook] alert-notification workflow not found');
      return res.status(404).json({ error: 'Workflow not found' });
    }

    // Trigger alert-notification workflow
    const workflowRun = await prisma.workflowRun.create({
      data: {
        workflowId: workflow.id,
        trigger: 'grafana_webhook',
        contextJson: context,
        status: 'PENDING',
      },
    });

    // Execute workflow asynchronously (notifier agent will send alert)
    workflowRunner.executeWorkflow(workflowRun.id).catch((error) => {
      console.error('[Webhook] Workflow execution error:', error);
    });

    res.status(200).json({ workflowRunId: workflowRun.id });
  } catch (error) {
    console.error('[Webhook] Error processing Grafana webhook:', error);
    res.status(500).json({ error: 'Internal server error' });
  }
});

export default router;
```

**Step 2: Register webhook router**

In `hub/orchestration/src/api/server.ts`:

```typescript
import webhooksRouter from './routes/webhooks';

// ... other routes ...
app.use('/api', webhooksRouter);
```

**Step 3: Commit**

```bash
git add src/api/routes/webhooks.ts src/api/server.ts
git commit -m "feat(observability): Add Grafana alert webhook receiver"
```

---

### Task 22: Create Notification Contact Point

**Files:**
- Create: `hub/observability/grafana/provisioning/alerting/contactpoints.yml`

**Step 1: Write contact point config**

```yaml
# hub/observability/grafana/provisioning/alerting/contactpoints.yml
apiVersion: 1

contactPoints:
  - orgId: 1
    name: webhook-notifier
    receivers:
      - uid: webhook_notifier
        type: webhook
        settings:
          url: http://host.docker.internal:9002/api/webhooks/grafana
          httpMethod: POST
```

**Step 2: Create notification policy**

Create: `hub/observability/grafana/provisioning/alerting/policies.yml`

```yaml
# hub/observability/grafana/provisioning/alerting/policies.yml
apiVersion: 1

policies:
  - orgId: 1
    receiver: webhook-notifier
    group_by: ['alertname', 'severity']
    group_wait: 30s
    group_interval: 5m
    repeat_interval: 4h
    routes:
      - receiver: webhook-notifier
        matchers:
          - severity = critical
        group_wait: 10s
        group_interval: 1m
        repeat_interval: 1h
      - receiver: webhook-notifier
        matchers:
          - severity = warning
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 4h
```

**Step 3: Commit**

```bash
git add observability/grafana/provisioning/alerting/
git commit -m "feat(observability): Add alert notification contact points"
```

---

### Task 23: Create Alert Notification Workflow

**Files:**
- Create: `hub/orchestration/scripts/create-alert-notification-workflow.ts`

**Step 1: Write workflow creation script**

```typescript
// hub/orchestration/scripts/create-alert-notification-workflow.ts

import { PrismaClient } from '@prisma/client';

const prisma = new PrismaClient();

async function createAlertNotificationWorkflow() {
  // Find notifier agent
  const notifierAgent = await prisma.agent.findFirst({
    where: { name: 'notifier' },
  });

  if (!notifierAgent) {
    throw new Error('notifier agent not found - run register-agents.sh first');
  }

  // Create workflow
  const workflow = await prisma.workflow.create({
    data: {
      name: 'alert-notification',
      description: 'Send alerts via notifier agent (Slack/Discord/Console)',
      trigger: 'manual',
      riskLevel: 'AUTO',
      nodes: [
        {
          id: 'notify',
          type: 'agent',
          agentId: notifierAgent.id,
          action: 'notify',
          inputs: {
            channel: '{{severity}}',
            message: 'ðŸš¨ Alert: {{alert_name}}\n\n{{message}}\n\nDashboard: {{dashboard_url}}',
          },
        },
      ],
      edges: [],
    },
  });

  console.log('âœ… Created alert-notification workflow:', workflow.id);
}

createAlertNotificationWorkflow()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('âŒ Error:', error);
    process.exit(1);
  });
```

**Step 2: Run script**

```bash
cd hub/orchestration
npx ts-node scripts/create-alert-notification-workflow.ts
```

Expected: "âœ… Created alert-notification workflow: [id]"

**Step 3: Commit**

```bash
git add scripts/create-alert-notification-workflow.ts
git commit -m "feat(observability): Add alert notification workflow"
```

---

### Task 24: Test End-to-End Alerting

**Files:**
- None (verification step)

**Step 1: Ensure all services are running**

```bash
cd hub
docker-compose -f docker-compose.observability.yml up -d
cd orchestration
npm run dev
```

**Step 2: Create alert-notification workflow**

```bash
npx ts-node scripts/create-alert-notification-workflow.ts
```

**Step 3: Trigger test failure to fire alert**

Create temporary script to trigger multiple failed workflows:

```bash
for i in {1..20}; do
  curl -X POST http://localhost:9002/api/workflows/test-failure/trigger \
    -H "Content-Type: application/json" \
    -d '{"contextJson": {}}' &
done
wait
```

(Note: Assumes test-failure workflow exists or create one that always fails)

**Step 4: Wait for alert to fire**

Alert threshold: 10% failure rate over 5 minutes
Expected: Alert fires within 5-6 minutes

**Step 5: Check Grafana alerting UI**

Open: http://localhost:3001/alerting/list
Expected: "Workflow Failure Rate" alert shows "Firing" state

**Step 6: Verify webhook received**

Check orchestration service logs:
```bash
cd hub/orchestration
npm run dev # Check console output
```

Expected: "[Webhook] Received Grafana alert: Workflow Failure Rate"

**Step 7: Verify notifier agent executed**

Query workflow runs:
```bash
curl http://localhost:9002/api/workflows/alert-notification/runs
```

Expected: Shows recent run with status SUCCESS and notifier agent output

**Step 8: Document verification**

```bash
git add -A
git commit -m "feat(observability): Phase 4 complete - Alerting working

- Alert rules configured and auto-provisioned
- Alerts fire within 1 minute of threshold breach
- Grafana webhook triggers alert-notification workflow
- Notifier agent delivers alert message
- End-to-end alerting flow verified"
```

---

### Task 25: Create Documentation

**Files:**
- Create: `hub/observability/README.md`

**Step 1: Write observability documentation**

```markdown
# CommandCenter Observability Stack

Production-grade observability for the orchestration service using OpenTelemetry, Grafana, Prometheus, and Tempo.

## Quick Start

### Start Observability Stack

```bash
cd hub
docker-compose -f docker-compose.observability.yml up -d
```

### Access Dashboards

- **Grafana**: http://localhost:3001 (admin/admin)
- **Prometheus**: http://localhost:9090
- **Tempo**: http://localhost:3200

### Stop Stack

```bash
docker-compose -f docker-compose.observability.yml down
```

## Architecture

```
Orchestration Service (TypeScript)
  â†’ OpenTelemetry SDK (auto-instrumentation)
  â†’ OTEL Collector (process & route)
  â†’ Tempo (traces) + Prometheus (metrics) + Loki (logs)
  â†’ Grafana (dashboards & alerts)
```

## Dashboards

### 1. Workflow Overview
- Workflows per minute
- Average workflow duration (p95)
- Active workflows
- Workflow success rate
- Top 10 slowest workflows

### 2. Agent Performance
- Agent execution heatmap
- Agent duration percentiles (p50, p95, p99)
- Agent failure rate by type
- Most expensive agents
- Agent retry trends

### 3. System Health
- API response times (p95 by endpoint)
- Database query latency
- Error rate (5xx errors)
- Node.js memory usage
- Event loop lag
- Active HTTP connections

### 4. Cost Tracking
- Agent execution minutes by type
- Workflow cost breakdown
- Monthly execution trends
- Cost per workflow type

## Metrics

### Workflow Metrics
- `workflow.runs.total` - Total workflow executions (counter)
- `workflow.duration` - Workflow execution time (histogram)
- `workflows.active` - Currently running workflows (gauge)
- `workflow.approval.wait_time` - Approval wait time (histogram)

### Agent Metrics
- `agent.runs.total` - Total agent executions (counter)
- `agent.duration` - Agent execution time (histogram)
- `agent.errors.total` - Agent execution failures (counter)
- `agent.retry.count` - Agent retry attempts (counter)

### System Metrics (auto-collected)
- `http.server.duration` - API endpoint latency
- `db.client.operation.duration` - Database query performance
- `process.runtime.nodejs.memory.heap` - Memory usage
- `process.runtime.nodejs.event_loop.lag` - Event loop lag

## Alerting

### Critical Alerts (PagerDuty/On-Call)
- **WorkflowFailureRate**: > 10% failures over 5 minutes
- **ServiceDown**: Orchestration service unavailable > 1 minute
- **HighErrorRate**: > 50 5xx errors/minute for 2 minutes

### Warning Alerts (Slack)
- **HighWorkflowDuration**: p95 > 5 minutes for 10 minutes
- **AgentFailureSpike**: > 5 failures in 10 minutes per agent
- **HighMemoryUsage**: > 80% memory for 5 minutes

### Alert Routing
Grafana â†’ Webhook â†’ `alert-notification` workflow â†’ Notifier agent â†’ Slack/Discord

## Configuration

### Environment Variables

```bash
# Orchestration service
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317  # OTEL Collector endpoint
```

### Data Retention
- **Prometheus**: 30 days
- **Tempo**: 7 days
- **Loki**: 7 days

### Resource Requirements
- CPU: 2 cores (development), 8 cores (production)
- Memory: 4GB (development), 16GB (production)
- Storage: ~20GB for 7-30 day retention

## Troubleshooting

### No traces appearing in Tempo

1. Check OTEL Collector logs:
   ```bash
   docker-compose -f docker-compose.observability.yml logs otel-collector
   ```

2. Verify orchestration service is sending traces:
   ```bash
   curl http://localhost:9002/health
   # Check for "[OpenTelemetry] Instrumentation started" in logs
   ```

3. Check OTEL Collector endpoint is reachable:
   ```bash
   nc -zv localhost 4317
   ```

### Metrics not appearing in Prometheus

1. Check Prometheus targets:
   Open http://localhost:9090/targets
   Expected: `orchestration-service` and `otel-collector` targets are UP

2. Verify Prometheus exporter is running:
   ```bash
   curl http://localhost:9464/metrics
   ```
   Expected: Returns OpenTelemetry metrics

### Dashboards not loading

1. Check Grafana logs:
   ```bash
   docker-compose -f docker-compose.observability.yml logs grafana
   ```

2. Verify dashboard files exist:
   ```bash
   ls hub/observability/grafana/dashboards/
   ```

3. Check provisioning config:
   ```bash
   cat hub/observability/grafana/provisioning/dashboards/dashboards.yml
   ```

## Development

### Adding New Metrics

1. Define metric in `src/metrics/workflow-metrics.ts`:
   ```typescript
   export const myMetric = meter.createCounter('my.metric', {
     description: 'My custom metric',
   });
   ```

2. Record metric in code:
   ```typescript
   import { myMetric } from '../metrics/workflow-metrics';
   myMetric.add(1, { label: 'value' });
   ```

3. Query in Prometheus:
   ```promql
   sum(rate(my_metric[5m])) by (label)
   ```

### Adding New Dashboards

1. Create dashboard in Grafana UI
2. Export dashboard JSON
3. Save to `hub/observability/grafana/dashboards/my-dashboard.json`
4. Restart Grafana:
   ```bash
   docker-compose -f docker-compose.observability.yml restart grafana
   ```

## References

- [OpenTelemetry Node.js Docs](https://opentelemetry.io/docs/instrumentation/js/)
- [Grafana Tempo Docs](https://grafana.com/docs/tempo/latest/)
- [Prometheus Query Basics](https://prometheus.io/docs/prometheus/latest/querying/basics/)
- [Design Document](../docs/plans/2025-11-19-phase-10-phase-5-observability-design.md)
```

**Step 2: Commit**

```bash
git add observability/README.md
git commit -m "docs(observability): Add comprehensive observability documentation"
```

---

### Task 26: Update PROJECT.md

**Files:**
- Modify: `docs/PROJECT.md`

**Step 1: Read current PROJECT.md**

```bash
head -300 docs/PROJECT.md
```

**Step 2: Update Phase 10 Phase 5 status**

Find the Phase 10 Phase 5 section and update:

```markdown
- âœ… **Phase 10: Phase 5** - Observability (OpenTelemetry, Prometheus, Grafana) **COMPLETE** (2025-11-19)
  * **Foundation**: Auto-instrumentation (HTTP, DB, external calls)
  * **Custom Spans**: Workflow and agent execution tracing
  * **Metrics**: 8 custom metrics (workflow runs, duration, agent performance)
  * **Dashboards**: 4 Grafana dashboards (Workflow, Agent, System, Cost)
  * **Alerting**: 6 alert rules + Grafana webhook â†’ notifier agent
  * **Components**: OTEL Collector, Tempo, Prometheus, Loki, Grafana
  * **Verified**: End-to-end tracing, metrics collection, alert delivery
  * **Files**: `hub/observability/`, `hub/orchestration/src/instrumentation.ts`, `hub/orchestration/src/metrics/`
```

**Step 3: Commit**

```bash
git add docs/PROJECT.md
git commit -m "docs: Mark Phase 10 Phase 5 (Observability) as complete"
```

---

### Task 27: Final Verification

**Files:**
- None (final verification)

**Step 1: Full stack startup test**

```bash
# Start observability stack
cd hub
docker-compose -f docker-compose.observability.yml up -d

# Wait for services to be ready
sleep 30

# Verify all services are up
docker-compose -f docker-compose.observability.yml ps
```

Expected: All 5 services show "Up" status

**Step 2: Start orchestration service**

```bash
cd hub/orchestration
npm run dev
```

Expected: Console shows "[OpenTelemetry] Instrumentation started"

**Step 3: Trigger test workflow**

```bash
curl -X POST http://localhost:9002/api/workflows/scan-and-notify/trigger \
  -H "Content-Type: application/json" \
  -d '{"contextJson": {"repository": "test-repo"}}'
```

**Step 4: Verify complete observability chain**

1. **Traces**: Open http://localhost:3200/explore
   - Search: service.name = "orchestration-service"
   - Expected: See workflow.execute span with agent.execute children

2. **Metrics**: Open http://localhost:9090
   - Query: `workflow_runs_total`
   - Expected: Counter incremented with correct labels

3. **Dashboards**: Open http://localhost:3001/dashboards
   - Open "Workflow Overview"
   - Expected: Shows workflow execution data in all panels

4. **Alerting**: Open http://localhost:3001/alerting/list
   - Expected: All alert rules loaded (6 rules)

**Step 5: Verify success criteria**

From design document section 7.4:

- âœ… All HTTP/DB operations auto-traced without code changes
- âœ… Custom workflow/agent spans with detailed attributes
- âœ… Dashboards show real-time workflow metrics
- âœ… Alerts fire within 1 minute of threshold breach (tested in Task 24)
- âœ… End-to-end trace from API request â†’ workflow â†’ agents â†’ response
- âœ… Metrics have low cardinality (check label combinations in Prometheus)
- âœ… Dashboard queries complete in < 2 seconds under load
- âœ… Observability stack uses < 500MB memory combined

Check memory usage:
```bash
docker stats --no-stream hub-otel-collector hub-tempo hub-prometheus-obs hub-loki hub-grafana-obs
```

**Step 6: Final commit**

```bash
git add -A
git commit -m "feat(observability): Phase 10 Phase 5 complete - Production-ready observability

Summary:
- OpenTelemetry auto-instrumentation for HTTP, DB, external calls
- Custom spans for workflow and agent execution
- 8 custom metrics (workflow/agent performance)
- 4 Grafana dashboards (Workflow, Agent, System, Cost)
- 6 alert rules (critical + warning)
- Grafana webhook â†’ alert-notification workflow
- End-to-end alerting via notifier agent
- Comprehensive documentation

Success criteria met:
âœ… Auto-tracing with zero code changes
âœ… Custom spans with rich attributes
âœ… Real-time dashboards
âœ… Sub-minute alert delivery
âœ… End-to-end trace visibility
âœ… Low cardinality metrics
âœ… Fast dashboard queries (< 2s)
âœ… Minimal resource usage (< 500MB)

Files changed:
- hub/orchestration/src/instrumentation.ts
- hub/orchestration/src/metrics/workflow-metrics.ts
- hub/orchestration/src/services/workflow-runner.ts
- hub/orchestration/src/api/routes/webhooks.ts
- hub/docker-compose.observability.yml
- hub/observability/ (config files + dashboards)
- docs/PROJECT.md

Design: docs/plans/2025-11-19-phase-10-phase-5-observability-design.md"
```

---

## Summary

**Total Tasks**: 27
**Estimated Time**: 10-12 hours

### Phase Breakdown

**Phase 1: Foundation** (Tasks 1-10) - 2-3 hours
- OpenTelemetry dependencies
- Instrumentation module
- Observability infrastructure (Docker Compose)
- Basic verification

**Phase 2: Custom Spans** (Tasks 11-14) - 2-3 hours
- Workflow execution spans
- Agent execution spans
- Custom metrics
- Trace verification

**Phase 3: Metrics & Dashboards** (Tasks 15-19) - 3-4 hours
- 4 Grafana dashboards
- Dashboard provisioning
- Verification

**Phase 4: Alerting** (Tasks 20-27) - 3-4 hours
- Alert rules
- Webhook receiver
- Alert notification workflow
- End-to-end testing
- Documentation

### Key Files Created

**TypeScript**:
- `hub/orchestration/src/instrumentation.ts` (117 lines)
- `hub/orchestration/src/metrics/workflow-metrics.ts` (47 lines)
- `hub/orchestration/src/api/routes/webhooks.ts` (50 lines)

**Configuration**:
- `hub/docker-compose.observability.yml` (140 lines)
- `hub/observability/otel-collector-config.yml` (50 lines)
- `hub/observability/prometheus.yml` (20 lines)
- `hub/observability/tempo.yml` (25 lines)
- `hub/observability/grafana/provisioning/` (5 files)

**Dashboards**:
- `hub/observability/grafana/dashboards/*.json` (4 files, ~2000 lines total)

**Documentation**:
- `hub/observability/README.md` (400 lines)

### Testing Strategy

Each phase includes verification steps:
- Phase 1: Verify auto-instrumentation working
- Phase 2: Verify custom spans appearing in Tempo
- Phase 3: Verify dashboards showing data
- Phase 4: Verify end-to-end alerting

### Next Steps (Phase 6+)

After Phase 5 is complete:
- **Phase 6**: Production Readiness
  - Additional agents (code-reviewer, patcher, compliance-checker)
  - Load testing (50+ concurrent workflows)
  - Security audit (RBAC, secrets management)
  - Documentation (runbooks, architecture docs)
