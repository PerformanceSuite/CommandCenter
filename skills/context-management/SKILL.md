---
name: context-management
description: Use proactively throughout sessions to maintain context under 50% capacity (100k tokens) via incremental cleanup, preventing expensive /compact operations
---

# Context Management Skill

**Goal**: Keep context usage under 50% capacity (100k/200k tokens) through continuous, non-disruptive optimization.

## Quick Decision Flowchart

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              START: Check Token Usage in System Reminder        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Calculate Usage % â”‚
                    â”‚  (tokens/200000)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                  â”‚                  â”‚
          â–¼                  â–¼                  â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  0-40%  â”‚       â”‚ 40-60%  â”‚       â”‚  60%+    â”‚
    â”‚ Healthy â”‚       â”‚ Warning â”‚       â”‚ Critical â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚                  â”‚
         â–¼                 â–¼                  â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Continue    â”‚  â”‚ Enable       â”‚  â”‚ Aggressive     â”‚
    â”‚ Normally    â”‚  â”‚ Optimization â”‚  â”‚ Cleanup        â”‚
    â”‚             â”‚  â”‚              â”‚  â”‚                â”‚
    â”‚ â€¢ Full      â”‚  â”‚ â€¢ Disable    â”‚  â”‚ â€¢ Thinking OFF â”‚
    â”‚   thinking  â”‚  â”‚   thinking   â”‚  â”‚ â€¢ Summarize    â”‚
    â”‚ â€¢ Normal    â”‚  â”‚ â€¢ Selective  â”‚  â”‚   everything   â”‚
    â”‚   reads     â”‚  â”‚   file reads â”‚  â”‚ â€¢ Filter all   â”‚
    â”‚             â”‚  â”‚ â€¢ Filter     â”‚  â”‚   outputs      â”‚
    â”‚             â”‚  â”‚   outputs    â”‚  â”‚ â€¢ Prune memory â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                                                 â–¼
                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                          â”‚   80%+ ?     â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚                 â”‚
                                        â–¼                 â–¼
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚  YES   â”‚      â”‚    NO    â”‚
                                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  EMERGENCY PROTOCOL   â”‚
                            â”‚                       â”‚
                            â”‚ 1. Complete task      â”‚
                            â”‚ 2. Commit changes     â”‚
                            â”‚ 3. Update memory      â”‚
                            â”‚ 4. Suggest /compact   â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Reference Card

| Scenario | Token Range | Strategy | Expected Savings | Action |
|----------|-------------|----------|------------------|--------|
| **Session Start** | 10-20k | MCP optimization | 15-25k tokens | Auto-disable unused MCPs |
| **Simple Task** | Any | Thinking suppression | 20-30% per response | \`<thinking_mode>disabled</thinking_mode>\` |
| **Large File Read** | Any | Selective reading | 50-80% per file | Use grep + offset/limit |
| **Command Output** | Any | Output filtering | 60-90% per command | Pipe to head/grep |
| **Task Complete** | 40k+ | Summarization | 10-15% | Summary instead of full output |
| **Mid-Session** | 80-100k | Memory pruning | 10-15k tokens | Clean old session entries |
| **Critical (60%+)** | 120k+ | Aggressive mode | 30-40% | All strategies combined |
| **Emergency (80%+)** | 160k+ | Compaction | Reset to ~10k | Save work + /compact |

## MANDATORY INVOCATION

**This skill is invoked automatically by \`/start\` command.**

You MUST follow its practices throughout the entire session:
- Thinking mode disabled by default (only enable for complex reasoning)
- Use \`gh\` CLI instead of GitHub MCP
- Selective file reading with offset/limit
- Filter command outputs with head/tail/grep

**DO NOT SKIP THESE PRACTICES** - they are mandatory, not optional.

## When to Use This Skill

**Proactive (Best):**
- At the start of every session (after /start) - AUTOMATICALLY INVOKED
- After completing major tasks (e.g., file writes, commits)
- Every 10-15 minutes during active development
- Before reading large files or generating long outputs

**Reactive (Fallback):**
- When system reminder shows >80k tokens
- When you notice slower responses
- Before complex tasks that will generate significant output

## Context Monitoring Protocol

### 1. Check Current Usage

When you see system reminders like:
\`\`\`
Token usage: 115584/200000; 84416 remaining
\`\`\`

Calculate percentage:
- **Current**: 115584 / 200000 = 58% âš ï¸
- **Threshold**: 50% = 100k tokens

### 2. Alert Thresholds

| Usage | Status | Action |
|-------|--------|--------|
| 0-40% | âœ… Healthy | Continue normally |
| 40-50% | ğŸŸ¡ Caution | Begin incremental cleanup |
| 50-60% | ğŸŸ  Warning | Active cleanup + notifications |
| 60-80% | ğŸ”´ Critical | Aggressive cleanup required |
| 80%+ | ğŸš¨ Emergency | Immediate compaction |

### 3. Communicate Status

When threshold exceeded, inform user:

\`\`\`
ğŸŸ¡ Context at 45% (90k/200k) - Starting incremental cleanup
ğŸŸ  Context at 55% (110k/200k) - Active cleanup in progress
ğŸ”´ Context at 65% (130k/200k) - Aggressive cleanup needed
ğŸš¨ Context at 85% (170k/200k) - Emergency compaction required
\`\`\`

## Incremental Cleanup Strategies

### Strategy 0: MCP Server Optimization (Automatic on /start, -15-25k tokens)

**When**: Automatically during \`/start\` command

**Problem**: MCP servers consume massive context even when unused:
- GitHub MCP: ~18k tokens (26 tools)
- Puppeteer MCP: ~4.7k tokens
- Brave Search MCP: ~1.4k tokens
- IDE MCP: ~1.3k tokens

**Action**: Autonomously disable unused MCPs based on project type

**Before/After Example**:
\`\`\`
Before: All MCPs loaded
- Token usage: 25,400 tokens (system + tools)
- 4 MCP servers active
- 40+ tools in context

After: Optimized for backend project
- Token usage: 6,800 tokens (system only)
- 1 MCP server active (GitHub structure only, use gh CLI)
- 6 tools in context

Savings: 18,600 tokens (73% reduction in overhead)
\`\`\`

**Workflow**:
1. Read \`~/.claude/mcp.json\`
2. Detect project type (backend/frontend/fullstack/etc)
3. Identify unused MCPs:
   - **Always disable**: brave-search (redundant with WebSearch)
   - **Backend projects**: Disable puppeteer, ide (unless Jupyter notebooks present)
   - **Frontend projects**: Keep all (may need browser automation)
   - **GitHub**: Always keep enabled (but use \`gh\` CLI via Bash instead of MCP tools)
4. Comment out unused MCP entries
5. Report optimization results

**Implementation**: Automatically runs during \`/start\`, reports results like:
\`\`\`
ğŸ”§ MCP Optimization: Disabled 2 unused servers (saves ~6.1k tokens)
  - brave-search: Redundant with WebSearch
  - puppeteer: Not needed for backend work
\`\`\`

**Note**: Use \`gh\` CLI via Bash instead of GitHub MCP tools to save an additional ~18k tokens per session (even though GitHub MCP stays enabled for structure).

### Strategy 1: Thinking Suppression (Instant, -20-30%)

**When**: Disable thinking mode by default. Only enable for complex tasks.

**Disable thinking for**:
- Executing clear plans (step-by-step implementation)
- Simple file operations (read, write, edit)
- Running tests or commands
- Code lookups or searches
- Following established patterns

**Enable thinking ONLY for**:
- Brainstorming or design decisions
- Reviewing complex plans for issues
- Debugging ambiguous errors
- Architecture or approach selection
- Trade-off analysis

**Action**: Add at start of response:
\`\`\`markdown
<thinking_mode>disabled</thinking_mode>
\`\`\`

**Before/After Example**:

\`\`\`
Before: Response with thinking enabled
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ <thinking>                                  â”‚
â”‚ The user wants to update the config file.   â”‚
â”‚ I should:                                   â”‚
â”‚ 1. Read the current file                   â”‚
â”‚ 2. Identify the section to modify          â”‚
â”‚ 3. Make the changes                        â”‚
â”‚ 4. Write it back                           â”‚
â”‚ This is straightforward...                 â”‚
â”‚ </thinking>                                â”‚
â”‚                                            â”‚
â”‚ I'll update the configuration file now.    â”‚
â”‚ [tool calls and response]                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Total: ~3,500 tokens

After: Response with thinking disabled
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ <thinking_mode>disabled</thinking_mode>     â”‚
â”‚                                            â”‚
â”‚ I'll update the configuration file now.    â”‚
â”‚ [tool calls and response]                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Total: ~1,200 tokens

Savings: 2,300 tokens (66% reduction per response)
Over 10 responses: 23,000 tokens saved (11.5% of total budget)
\`\`\`

**Impact**:
- Reduces token usage by 20-30% immediately
- No quality loss for straightforward tasks
- Re-enable when needed: \`<thinking_mode>enabled</thinking_mode>\`

**Example**:
\`\`\`markdown
<thinking_mode>disabled</thinking_mode>

Executing Task 3: Update config file...
\`\`\`

### Strategy 2: Output Summarization (Fast, -10-15%)

**When**: After completing discrete tasks

**Action**: Instead of showing full tool outputs, summarize:

**Before/After Example**:

\`\`\`
Before: Full output quoted
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Read the configuration file:               â”‚
â”‚                                            â”‚
â”‚ <file contents - 500 lines>                â”‚
â”‚ [entire file quoted]                       â”‚
â”‚ </file contents>                           â”‚
â”‚                                            â”‚
â”‚ I've read the entire file. Let me now...   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Total: ~8,200 tokens

After: Summarized output
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Read config.py (500 lines)                 â”‚
â”‚ Found 3 relevant sections:                 â”‚
â”‚ - Database config (lines 45-67)            â”‚
â”‚ - API settings (lines 120-145)             â”‚
â”‚ - Cache config (lines 230-256)             â”‚
â”‚                                            â”‚
â”‚ Proceeding with update...                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Total: ~1,100 tokens

Savings: 7,100 tokens (87% reduction per file operation)
\`\`\`

âŒ **Don't** (wasteful):
\`\`\`
<file contents - 500 lines>
I've read the entire file. Let me now...
\`\`\`

âœ… **Do** (efficient):
\`\`\`
Read config.py (500 lines) - found 3 relevant sections
Proceeding with update...
\`\`\`

### Strategy 3: Selective File Reading (Moderate, -15-25%)

**When**: Before reading files

**Actions**:
1. Use \`offset\` and \`limit\` parameters when possible
2. Grep/search before full reads
3. Read only changed sections for updates

**Before/After Example**:

\`\`\`
Before: Full file read
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Read(file_path="app/models/user.py")       â”‚
â”‚ â†’ Returns entire file (850 lines)          â”‚
â”‚                                            â”‚
â”‚ Token usage: 12,400 tokens                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

After: Targeted read with grep + offset/limit
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Grep(pattern="class User", path="app/      â”‚
â”‚      models/user.py")                      â”‚
â”‚ â†’ Found at line 145                        â”‚
â”‚                                            â”‚
â”‚ Read(file_path="app/models/user.py",      â”‚
â”‚      offset=145, limit=50)                â”‚
â”‚ â†’ Returns only relevant class (50 lines)   â”‚
â”‚                                            â”‚
â”‚ Token usage: 1,800 tokens                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Savings: 10,600 tokens (85% reduction per large file)
\`\`\`

âŒ **Don't**:
\`\`\`python
Read(file_path="large_file.py")  # Reads entire file
\`\`\`

âœ… **Do**:
\`\`\`python
Grep(pattern="class MyClass", path="large_file.py")  # Find location first
Read(file_path="large_file.py", offset=100, limit=50)  # Read targeted section
\`\`\`

### Strategy 4: Tool Output Filtering (Fast, -5-10%)

**When**: Running commands with verbose output

**Actions**:
1. Pipe to \`head\` or \`tail\` for previews
2. Use \`grep\` to filter relevant lines
3. Suppress unnecessary output

**Before/After Example**:

\`\`\`
Before: Full verbose test output
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ $ pytest tests/ -v                         â”‚
â”‚                                            â”‚
â”‚ tests/test_auth.py::test_login PASSED     â”‚
â”‚ tests/test_auth.py::test_logout PASSED    â”‚
â”‚ tests/test_api.py::test_get_user PASSED   â”‚
â”‚ [... 200 more test lines ...]             â”‚
â”‚ tests/test_db.py::test_query PASSED       â”‚
â”‚                                            â”‚
â”‚ ========== 247 passed in 4.32s ==========  â”‚
â”‚                                            â”‚
â”‚ Token usage: 6,800 tokens                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

After: Filtered output
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ $ pytest tests/ -v 2>&1 | grep -E         â”‚
â”‚   "PASSED|FAILED|ERROR" | head -20         â”‚
â”‚                                            â”‚
â”‚ tests/test_auth.py::test_login PASSED     â”‚
â”‚ tests/test_auth.py::test_logout PASSED    â”‚
â”‚ [... 15 more lines ...]                   â”‚
â”‚ tests/test_api.py::test_update PASSED     â”‚
â”‚                                            â”‚
â”‚ $ pytest tests/ -v 2>&1 | tail -3         â”‚
â”‚ ========== 247 passed in 4.32s ==========  â”‚
â”‚                                            â”‚
â”‚ Token usage: 950 tokens                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Savings: 5,850 tokens (86% reduction per test run)
\`\`\`

âŒ **Don't**:
\`\`\`bash
pytest tests/ -v  # Full verbose output
\`\`\`

âœ… **Do**:
\`\`\`bash
pytest tests/ -v 2>&1 | grep -E "PASSED|FAILED|ERROR" | head -20
\`\`\`

### Strategy 5: Memory File Optimization (Moderate, -10-15%)

**When**: At task completion or every 30 minutes

**Actions**:
1. Remove outdated session entries from \`.claude/memory.md\`
2. Consolidate repetitive information
3. Move details to session logs instead of memory

**Before/After Example**:

\`\`\`
Before: Verbose memory with full details
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ## Session: 2025-10-20                     â”‚
â”‚ Started: 9:00 AM                           â”‚
â”‚ Work: Implemented user authentication      â”‚
â”‚ - Created login endpoint                   â”‚
â”‚ - Added JWT token generation               â”‚
â”‚ - Implemented token validation middleware  â”‚
â”‚ - Added refresh token rotation             â”‚
â”‚ - Created logout endpoint                  â”‚
â”‚ Status: Completed                          â”‚
â”‚ Next: Start working on authorization       â”‚
â”‚                                            â”‚
â”‚ ## Session: 2025-10-21                     â”‚
â”‚ [... similar verbose details ...]          â”‚
â”‚                                            â”‚
â”‚ ## Session: 2025-10-22                     â”‚
â”‚ [... similar verbose details ...]          â”‚
â”‚                                            â”‚
â”‚ Token usage: 14,200 tokens                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

After: Optimized memory with references
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ## Session: 2025-10-26 (LATEST)            â”‚
â”‚ Work: Completed RAG service rewrite        â”‚
â”‚ Next: Dagger integration                   â”‚
â”‚ Details: .claude/logs/sessions/            â”‚
â”‚          2025-10-26_203915.md              â”‚
â”‚                                            â”‚
â”‚ ## Session: 2025-10-25                     â”‚
â”‚ Work: API optimization                     â”‚
â”‚ Details: .claude/logs/sessions/            â”‚
â”‚          2025-10-25_143022.md              â”‚
â”‚                                            â”‚
â”‚ Token usage: 1,800 tokens                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Savings: 12,400 tokens (87% reduction in memory overhead)
\`\`\`

**Example**:
\`\`\`markdown
## Session: 2025-10-26 (LATEST)
Work: Completed RAG service rewrite
Next: Dagger integration
Details: See .claude/logs/sessions/2025-10-26_203915.md
\`\`\`

### Strategy 6: Codebase Survey (For Multi-File Reviews)

**When**: Reviewing multiple files to identify patterns (e.g., router audit, service review)

**Problem**: Reading every file fully exhausts context before completing the review.

**Actions**:
1. Get inventory: `ls -la`, `wc -l *`, `grep -c "pattern" *`
2. Sample strategically: Read 20-30% of files fully, grep-sample remainder
3. Identify patterns: Common imports, naming conventions, structures
4. Deep-dive selectively: Full read only for unique/critical files
5. Document as you go: Write summaries incrementally

**Token Savings**: 40-60% for large multi-file reviews

**Example - Reviewing 29 Router Files:**
```bash
# Step 1: Get inventory
ls backend/app/routers/*.py | wc -l  # 29 files
wc -l backend/app/routers/*.py | sort -rn | head -5  # Find largest

# Step 2: Sample endpoints without full reads
grep -h "^@router\." backend/app/routers/*.py | wc -l  # 200+ endpoints

# Step 3: Full read key files only (20-30%)
# Read: auth.py, projects.py, graph.py, batch.py (critical)
# Grep-sample: remaining 25 files

# Step 4: Cross-file pattern detection
grep -r "from.*services" backend/app/routers/ | cut -d: -f2 | sort | uniq -c

# Result: 60k tokens used vs 140k estimated (57% savings)
```

### Strategy 7: Strategic Deep-Dive Criteria

**When**: Deciding whether to read a file fully or grep-sample it.

**Full Read If:**
- Core functionality or critical path
- Complex business logic
- Security-sensitive (auth, payments)
- File you need to modify
- Unique patterns not seen in other files

**Grep-Sample If:**
- Similar to other files already read
- Boilerplate or standard CRUD
- Large file (>500 lines) for quick overview
- Pattern already understood from other examples

**Rule of Thumb**: Read 20-30% of files fully, sample 70-80%

**Example Decision Tree:**
```
File: backend/app/routers/users.py (400 lines)
â”œâ”€â”€ Is it critical path? â†’ Read auth.py, projects.py
â”œâ”€â”€ Does it follow patterns from files already read? â†’ Grep-sample
â”œâ”€â”€ Am I modifying it? â†’ Full read
â””â”€â”€ Is it boilerplate CRUD? â†’ Grep endpoints only
```

### Strategy 8: Diminishing Returns Guidance

**When**: Deciding when to stop exploring and start summarizing

**Stopping Criteria for Large Reviews:**
- **Pattern clarity**: Can you describe the architecture confidently?
- **Coverage**: Have you examined 20-30% of code deeply?
- **Uniqueness**: Are new files showing new patterns or repeating existing?
- **Time**: After 30 min exploration, evaluate if additional reads add value

**Signs You Have Enough:**
- You can predict file contents before reading
- Same patterns appearing across multiple files
- No new concepts in last 3-5 files examined
- Confident you understand the architecture

**Signs You Need More:**
- Can't explain how pieces connect
- New patterns still emerging
- Critical files not yet examined
- Uncertainty about key functionality

### Strategy 9: Output Efficiency (Documentation Writing)

**When**: Creating large reports, summaries, or documentation

**Actions**:
- Write directly to files (not in chat) for long documents
- Use tables and lists (more compact than prose)
- Reference line numbers instead of code quotes
- Group related items to reduce repetition

**Before/After Example**:
```
Before: Full code quotes in response
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Here's the authentication implementation:   â”‚
â”‚                                             â”‚
â”‚ ```python                                   â”‚
â”‚ def login(email: str, password: str):       â”‚
â”‚     user = get_user_by_email(email)         â”‚
â”‚     if not user:                            â”‚
â”‚         raise HTTPException(404)            â”‚
â”‚     if not verify_password(password, ...):  â”‚
â”‚         raise HTTPException(401)            â”‚
â”‚     return create_token(user)               â”‚
â”‚ ```                                         â”‚
â”‚                                             â”‚
â”‚ Token usage: 2,400 tokens                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

After: Line references
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Authentication implementation:              â”‚
â”‚ - Login: auth.py:45-67 (validates, tokens) â”‚
â”‚ - Refresh: auth.py:70-85 (JWT rotation)    â”‚
â”‚ - Logout: auth.py:88-95 (client-side)      â”‚
â”‚                                             â”‚
â”‚ Token usage: 350 tokens                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Savings: 2,050 tokens (85% reduction per reference)
```

### Strategy 10: Cross-File Analysis Commands

**When**: Finding patterns across multiple files without reading each fully

```bash
# Count endpoints per router file
for file in backend/app/routers/*.py; do
  echo "$(basename $file): $(grep -c '^@router\.' $file 2>/dev/null || echo 0)"
done

# List all unique HTTP methods used
grep -h '^@router\.' backend/app/routers/*.py | \
  sed 's/@router\.\([^(]*\).*/\1/' | sort -u

# Find common imports across services
grep -h "^from" backend/app/services/*.py | sort | uniq -c | sort -rn | head -10

# Detect file size distribution
find backend/app/routers -name "*.py" -exec wc -l {} + | sort -rn | head -10

# Check for patterns indicating complexity
grep -c "async def" backend/app/routers/*.py | sort -t: -k2 -rn | head -5
```

**Token Savings**: These commands give you architectural understanding at ~100-500 tokens vs 10-50k tokens for reading all files.

### Strategy 11: Conversation Pruning (Aggressive, -30-40%)

**When**: Critical threshold (60%+)

**Action**: Suggest to user:
\`\`\`
ğŸ”´ Context at 65% - Recommend conversation pruning:

Option 1: Continue with current task (5-10 min remaining)
Option 2: Complete task, then use /compact to reset context
Option 3: Start new session (save work first)

Current task will complete in ~X messages. Proceed?
\`\`\`

## Real Session Example: Optimization in Action

\`\`\`
Session Start: 10:00 AM
Initial tokens: 8,200 (system + tools)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

10:15 AM - After Reading 3 Files
Token usage: 32,400/200000 (16%)
âœ… Status: Healthy

Response:
"Read 3 configuration files, identified settings to update"
[No optimization needed yet]

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

10:35 AM - After Running Tests
Token usage: 78,600/200000 (39%)
âœ… Status: Healthy, approaching caution

Response (normal):
"Tests passing. Proceeding with next feature..."

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

10:50 AM - Multiple File Operations
Token usage: 92,800/200000 (46%)
ğŸŸ¡ Status: CAUTION - Begin optimization

Agent response:
"ğŸŸ¡ Context at 46% (92k/200k) - Starting incremental cleanup"

Optimizations enabled:
âœ“ Thinking mode: disabled for routine tasks
âœ“ File reading: using grep + offset/limit
âœ“ Command output: filtered with head/grep

Response example:
<thinking_mode>disabled</thinking_mode>

Grep found DatabaseConfig at line 245.
Reading lines 245-295 (50 lines).
Updated cache_timeout setting.

[Instead of quoting full 850-line file]

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

11:10 AM - Continued Development
Token usage: 104,200/200000 (52%)
ğŸŸ  Status: WARNING - Active cleanup

Agent response:
"ğŸŸ  Context at 52% (104k/200k) - Active cleanup in progress"

Additional optimizations:
âœ“ Memory file: pruned old sessions (saved 8.2k tokens)
âœ“ Output: summaries only, no full quotes
âœ“ Tests: filtered to show only failures + summary

Response example:
<thinking_mode>disabled</thinking_mode>

Ran pytest (247 tests) â†’ All passed in 4.32s
Updated API endpoint - changes committed.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

11:25 AM - Task Completion
Token usage: 118,400/200000 (59%)
ğŸŸ  Status: WARNING - Task nearly complete

Agent response:
"ğŸŸ  Context at 59% (118k/200k)

Task complete! Recommend:
1. Commit all work âœ…
2. Update memory âœ…
3. Start fresh session for next major feature

Type /end to close this session cleanly."

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Session Summary:
Duration: 1 hour 25 minutes
Work completed:
  - Updated 12 files
  - Added 3 new features
  - Ran test suite 5 times
  - Made 4 commits

Context management:
  - Started: 8.2k tokens (4%)
  - Ended: 118.4k tokens (59%)
  - Peak optimization: Saved ~35k tokens through strategies
  - Without optimization: Would have hit 153k tokens (76%)
    â†’ Emergency compaction required
  - With optimization: Completed full session comfortably

Result: âœ… Successful session without disruption
\`\`\`

## Continuous Background Optimization

### Every Response Checklist

Before generating each response:

1. â˜ Check if system reminder shows token usage
2. â˜ If >40%, enable thinking_mode=disabled
3. â˜ Summarize instead of quoting full outputs
4. â˜ Use selective file reading
5. â˜ Filter tool outputs to essentials

### After Each Major Task

After file writes, commits, or task completion:

1. â˜ Note tokens saved via optimization
2. â˜ Update user if crossing threshold
3. â˜ Clean memory file if >300 lines
4. â˜ Compress session logs

## Prevention Best Practices

### File Operations

**Reading**:
- Use Grep before Read to locate sections
- Use offset/limit for large files
- Read only what's needed for the task

**Writing**:
- Don't re-read files you just wrote
- Trust Write/Edit tool success messages
- Avoid unnecessary verification reads

### Tool Selection: Use \`gh\` CLI Instead of GitHub MCP

**GitHub MCP Cost**: 26 tools Ã— ~700 tokens = ~18k tokens loaded every session

**Alternative**: \`gh\` CLI via Bash tool (0 token overhead)

**Before/After Example**:

\`\`\`
Before: Using GitHub MCP
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Session startup:                           â”‚
â”‚ - Load 26 GitHub MCP tools                 â”‚
â”‚ - Tool definitions: ~18,000 tokens         â”‚
â”‚                                            â”‚
â”‚ Create PR:                                 â”‚
â”‚ mcp__github__create_pull_request(...)      â”‚
â”‚                                            â”‚
â”‚ Total overhead: 18,000 tokens (9% budget)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

After: Using gh CLI via Bash
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Session startup:                           â”‚
â”‚ - No additional tools loaded               â”‚
â”‚ - Tool definitions: 0 tokens               â”‚
â”‚                                            â”‚
â”‚ Create PR:                                 â”‚
â”‚ gh pr create --title "feat: Add feature"  â”‚
â”‚   --body "Description"                     â”‚
â”‚                                            â”‚
â”‚ Total overhead: 0 tokens                   â”‚
â”‚                                            â”‚
â”‚ Savings: 18,000 tokens (9% of total       â”‚
â”‚          budget freed for actual work)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

âœ… **Do** (0 MCP tokens):
\`\`\`bash
# Create PR
gh pr create --title "feat: Add feature" --body "Description"

# List issues
gh issue list --label bug --state open

# Get PR status
gh pr view 123 --json state,checks

# Create branch
gh repo clone owner/repo && git checkout -b feature

# Merge PR
gh pr merge 123 --squash
\`\`\`

âŒ **Don't** (loads 26 tools, 18k tokens):
\`\`\`python
mcp__github__create_pull_request(...)  # Wastes tokens
mcp__github__list_issues(...)
mcp__github__get_pull_request_status(...)
\`\`\`

**When MCP is Better**:
- Rare cases where structured JSON output is critical
- Multi-file operations that \`gh\` doesn't support well

**Token Savings**: ~18k tokens (9% of budget) per session

### Command Execution

**Bash**:
- Pipe to \`head -20\` for previews
- Use \`grep\` to filter output
- Combine related commands (fewer tool calls)

**Tests**:
- Run specific tests, not full suites
- Filter output to failures only
- Use \`-q\` (quiet) mode when appropriate

### Communication

**Explanations**:
- Be concise but clear
- Use bullet points over paragraphs
- Save detailed docs to files, not responses

**Code Examples**:
- Show diffs, not full files
- Use "..." to indicate omitted sections
- Reference line numbers instead of quoting

## Emergency Compaction Protocol

### When: 80%+ Usage

**Immediate Actions**:

1. **Complete Current Task** (if <5 min):
   \`\`\`
   ğŸš¨ Context at 85% - Completing current task then compacting
   \`\`\`

2. **Save State**:
   - Commit all changes
   - Update memory.md with progress
   - Note next steps clearly

3. **Inform User**:
   \`\`\`
   ğŸš¨ Emergency: Context at 85% (170k/200k)

   Action: Use /compact to reset conversation

   Progress saved:
   - All changes committed
   - Memory updated with next steps
   - Ready to continue after compaction
   \`\`\`

4. **User Decision**:
   - Wait for user to run \`/compact\`
   - OR continue with extreme caution (thinking disabled, minimal output)

## Context Estimation (When No System Reminder)

**Rough estimates**:
- Average response: ~1-2k tokens
- File read (500 lines): ~3-5k tokens
- Bash output (full): ~2-4k tokens
- Thinking block (long): ~1-3k tokens

**Track mentally**:
- Start of session: ~5-10k (system prompts)
- Every 10 responses: +10-20k
- Each file read: +3-5k
- Each command: +2-4k

**Alert at estimated**:
- 20 responses = ~40-50k (check status)
- 40 responses = ~80-100k (enable cleanup)
- 60 responses = ~120-150k (aggressive cleanup)

## Success Metrics

**Target**: Maintain <50% context throughout session

**Measurements**:
- Check token usage every 10-15 minutes
- Alert user proactively at thresholds
- Optimize without disrupting flow
- Only suggest /compact as last resort

**Good Session**:
- Never exceeded 60%
- Used incremental strategies
- Completed 2+ hours of work
- No forced interruptions

**Excellent Session**:
- Stayed under 50% entire time
- Thinking mode toggled strategically
- Selective reads and filtered outputs
- User didn't notice optimization

## Token Savings Summary

| Strategy | Timing | Per-Use Savings | Session Impact |
|----------|--------|-----------------|----------------|
| MCP Optimization | Session start | 15-25k tokens | 7-12% budget |
| Thinking Suppression | Per response | 2-3k tokens | 20-30% per response |
| Selective Reading | Per file | 8-15k tokens | 70-85% per file |
| Output Filtering | Per command | 3-8k tokens | 60-90% per command |
| Output Summarization | Per task | 5-10k tokens | 10-15% overall |
| Memory Pruning | Mid-session | 8-15k tokens | 4-7% budget |
| Codebase Survey | Multi-file reviews | 30-80k tokens | 40-60% per review |
| Strategic Deep-Dive | Per decision | 5-30k tokens | 50-80% avoided |
| Output Efficiency | Documentation | 10-30k tokens | 50-85% per doc |
| Cross-File Analysis | Architecture | 10-50k tokens | 90%+ avoided |
| **Combined Effect** | **Full session** | **~50-100k tokens** | **2-3x session capacity** |

**Example Math**:
- Without optimization: 180k tokens after 90 minutes â†’ Emergency compaction
- With optimization: 110k tokens after 90 minutes â†’ Continue working
- **Result**: 70k tokens saved = 35% more productive time per session

## Example Workflow

### Beginning of Session (10k tokens)
\`\`\`
âœ… Context healthy at 5% (10k/200k)
Proceeding with normal operation
\`\`\`

### After 15 Minutes (45k tokens)
\`\`\`
âœ… Context at 22% (45k/200k) - All good
Continuing with full thinking enabled
\`\`\`

### After 30 Minutes (85k tokens)
\`\`\`
ğŸŸ¡ Context at 42% (85k/200k)

Optimization enabled:
- Thinking mode: disabled for routine tasks
- File reading: selective with grep
- Command output: filtered to essentials

Continuing work...
\`\`\`

### After 45 Minutes (110k tokens)
\`\`\`
ğŸŸ  Context at 55% (110k/200k)

Active cleanup:
- Thinking: disabled except complex tasks
- Memory: pruned old sessions
- Outputs: summaries only

Recommend completing current task (RAG integration)
then starting fresh session. ETA: 10 minutes.
\`\`\`

### Task Complete (120k tokens)
\`\`\`
ğŸŸ  Context at 60% (120k/200k)

Task complete! Recommend:
1. Commit all work âœ…
2. Update memory âœ…
3. Start fresh session for next task

Type /end to close this session cleanly.
\`\`\`

## Anti-Patterns to Avoid

âŒ **Don't**:
- Read entire files when you need 10 lines
- Show full bash output when summary works
- Keep thinking enabled for simple tasks
- Quote large sections in responses
- Re-read files you just wrote

âœ… **Do**:
- Use grep/search to find, then read selectively
- Filter outputs with head/tail/grep
- Toggle thinking mode based on task complexity
- Reference files by path:line instead of quoting
- Trust tool success without verification

## Integration with Other Skills

**Works with**:
- \`verification-before-completion\`: Verify efficiently (selective reads)
- \`test-driven-development\`: Run specific tests, not full suite
- \`systematic-debugging\`: Use targeted investigation, not broad searches
- \`brainstorming\`: Disable thinking for idea capture
- \`writing-plans\`: Save detailed plans to files, not responses

**Complements**:
- USS /end command: Clean session wrap-up
- Memory rotation: Auto-archive when >500 lines
- Session logs: Details in logs, summaries in responses

---

**Remember**: The best context management is invisible to the user. Optimize continuously, alert proactively, and only disrupt when absolutely necessary.
